import torch
import numpy as np
import os
from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# âœ… å…³é”®ä¿®æ”¹ï¼šç›´æ¥ä» NN2_1 å¯¼å…¥æ‰€æœ‰æ ¸å¿ƒç»„ä»¶
# ç¡®ä¿ä½ çš„ NN2_1.py å°±åœ¨æ—è¾¹
from NN2_1 import create_spill_adapted_model, train_spill_model


# ==========================================
# 1. ç‰¹å¾å·¥ç¨‹ (æ³¢å½¢ç‰¹å¾æå–)
# ==========================================
def extract_features(X):
    """
    è¾“å…¥ X: [N, 15, 4] (COD, pH, DO, Vel) -> è¾“å‡º: [N, 8]
    é’ˆå¯¹æ— äººèˆ¹æ¨¡å¼ï¼Œæ–œç‡(Slope)ç‰¹å¾å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºå®ƒæŒ‡ç¤ºäº†ç¦»æºå¤´çš„æ–¹å‘
    """
    N, L, F = X.shape
    cod_seq, ph_seq, do_seq, vel_seq = X[:, :, 0], X[:, :, 1], X[:, :, 2], X[:, :, 3]

    # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
    cod_max = np.max(cod_seq, axis=1)
    cod_mean = np.mean(cod_seq, axis=1)
    cod_std = np.std(cod_seq, axis=1)
    ph_mean = np.mean(ph_seq, axis=1)
    do_mean = np.mean(do_seq, axis=1)
    vel_mean = np.mean(vel_seq, axis=1)

    # å½¢çŠ¶ç‰¹å¾ (æ–œç‡) - åœ¨ç©ºé—´æ¨¡å¼ä¸‹ï¼Œæ–œç‡ä»£è¡¨æµ“åº¦æ¢¯åº¦
    seq_index = np.arange(L)
    cod_slope = np.zeros(N)
    for i in range(N):
        try:
            # ç®€å•çš„çº¿æ€§æ‹Ÿåˆè·å–æ¢¯åº¦
            cod_slope[i] = np.polyfit(seq_index, cod_seq[i, :], 1)[0]
        except:
            cod_slope[i] = 0.0

    cod_range = cod_max - np.min(cod_seq, axis=1)

    feature_list = [cod_max, cod_mean, cod_std, cod_slope, cod_range, ph_mean, do_mean, vel_mean]
    return np.hstack([f.reshape(-1, 1) for f in feature_list])


# ==========================================
# 2. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# ==========================================
def prepare_data():
    # è¿™é‡Œè¯»å–çš„æ˜¯ä¸Šä¸€è½®ç”Ÿæˆçš„â€œæ— äººèˆ¹â€æ•°æ®
    path = 'boat_survey_long_dataset.npz'

    # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœæ²¡ç”Ÿæˆæ–°æ•°æ®ï¼Œè¿˜æ˜¯è¯»æ—§çš„è¯•è¯•
    if not os.path.exists(path):
        print(f"âš ï¸ æœªæ‰¾åˆ° {path}ï¼Œå°è¯•è¯»å–æ—§ç‰ˆ truck_spill_dataset.npz...")
        path = 'truck_spill_dataset.npz'

    if not os.path.exists(path):
        raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ•°æ®é›†ï¼è¯·å…ˆè¿è¡Œ generate_boat.py æˆ– generate2_2.py")

    print(f"ğŸ“¦ åŠ è½½æ•°æ®é›†: {path} ...")
    data = np.load(path)
    X_raw = data['sequences']  # [N, 15, 4]
    y_raw = data['targets']  # [Source, Distance]

    # ç»´åº¦æ£€æŸ¥ (ç¡®ä¿æ˜¯ [N, 15, 4])
    if X_raw.shape[1] == 4:  # å¦‚æœæ˜¯ [N, 4, 15]
        X = X_raw.transpose(0, 2, 1)
    else:
        X = X_raw

    print("ğŸ”§ æå–æ³¢å½¢å·¥ç¨‹ç‰¹å¾...")
    eng_features = extract_features(X)

    # Padding åˆ° 44ç»´ (ä¸ºäº†å…¼å®¹æ¨¡å‹é»˜è®¤è®¾ç½®)
    target_dim = 44
    if eng_features.shape[1] < target_dim:
        padding = np.zeros((len(X), target_dim - eng_features.shape[1]))
        eng_features = np.hstack([eng_features, padding])

    # æ•°æ®åˆ’åˆ†
    distances = y_raw[:, 1]
    # æ ¹æ®è·ç¦»åˆ†æ¡¶ï¼Œç”¨äºåˆ†å±‚é‡‡æ · (0-2km, 2-10km, 10-30km)
    # èˆ¹æµ‹æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬æ›´å…³æ³¨è¿‘åœºæ¢¯åº¦ï¼Œæ‰€ä»¥åˆ†æ¡¶é˜ˆå€¼è®¾å°ä¸€ç‚¹
    bucket_labels = np.digitize(distances, [2.0, 10.0, 30.0])

    # Split: Train(70%) / Val(15%) / Test(15%)
    X_train, X_test, eng_train, eng_test, y_train, y_test, b_train, b_test = train_test_split(
        X, eng_features, y_raw, bucket_labels, test_size=0.15, random_state=42, stratify=bucket_labels)
    X_train, X_val, eng_train, eng_val, y_train, y_val, b_train, b_val = train_test_split(
        X_train, eng_train, y_train, b_train, test_size=0.15, random_state=42, stratify=b_train)

    # æ ‡å‡†åŒ– (Scaling)
    for i in range(4):  # 4ä¸ªé€šé“åˆ†åˆ«æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_train[:, :, i] = scaler.fit_transform(X_train[:, :, i])
        X_val[:, :, i] = scaler.transform(X_val[:, :, i])
        X_test[:, :, i] = scaler.transform(X_test[:, :, i])

    eng_scaler = StandardScaler()
    eng_train = eng_scaler.fit_transform(eng_train)
    eng_val = eng_scaler.transform(eng_val)
    eng_test = eng_scaler.transform(eng_test)

    # ç›®æ ‡å€¼ Log å˜æ¢
    source_scaler = StandardScaler()
    y_src_train = source_scaler.fit_transform(np.log1p(y_train[:, 0:1])).flatten()
    y_src_val = source_scaler.transform(np.log1p(y_val[:, 0:1])).flatten()
    y_src_test = source_scaler.transform(np.log1p(y_test[:, 0:1])).flatten()

    print(f"âœ… æ•°æ®å‡†å¤‡å°±ç»ª: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}")
    return {
        'train': (X_train, eng_train, y_src_train, y_train[:, 1], b_train),
        'val': (X_val, eng_val, y_src_val, y_val[:, 1], b_val),
        'test': (X_test, eng_test, y_src_test, y_test[:, 1], b_test),
        'source_scaler': source_scaler
    }


# ==========================================
# 3. æ•°æ®åŠ è½½å™¨ (è¡¥å…¨ç¼ºå¤±éƒ¨åˆ†)
# ==========================================
def create_loaders(data_dict, batch_size=256):
    train_data = data_dict['train']
    # åŠ æƒé‡‡æ · (è§£å†³æ ·æœ¬ä¸å‡è¡¡)
    # np.bincount å¯èƒ½é‡åˆ°ç©ºçš„æ¡¶å¯¼è‡´é•¿åº¦ä¸å¯¹ï¼Œè¿™é‡ŒåŠ ä¸ªç®€å•çš„å®¹é”™
    buckets = train_data[4]
    if len(buckets) == 0:
        raise ValueError("è®­ç»ƒé›†ä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºåŠ è½½å™¨ï¼")

    counts = np.bincount(buckets)
    # é˜²æ­¢é™¤ä»¥0
    class_weights = 1.0 / np.maximum(counts, 1)
    # ç”Ÿæˆæ¯ä¸ªæ ·æœ¬çš„æƒé‡
    weights = class_weights[buckets]

    sampler = WeightedRandomSampler(weights, len(weights), replacement=True)

    # ã€ä¿®æ­£ç‚¹ã€‘è¿™é‡Œå‚æ•°åæ”¹æˆäº† samplerï¼Œä¸ä¸‹é¢è°ƒç”¨æ—¶ä¿æŒä¸€è‡´
    def make_loader(d, sampler=None, shuffle=False):
        # d æ˜¯ä¸€ä¸ª tuple: (X, eng, y_src, y_dist, y_bucket)
        tensors = [
            torch.FloatTensor(d[0]),
            torch.FloatTensor(d[1]),
            torch.FloatTensor(d[2]),
            torch.FloatTensor(d[3]),
            torch.LongTensor(d[4])
        ]

        # Windowsä¸‹å¤šè¿›ç¨‹è®¾ä¸º0ï¼ŒLinuxå¯è®¾ä¸º4
        import platform
        workers = 0 if platform.system() == 'Windows' else 4

        return DataLoader(
            TensorDataset(*tensors),
            batch_size=batch_size,
            sampler=sampler,  # è¿™é‡ŒåŒ¹é…å‚æ•°å
            shuffle=shuffle,
            num_workers=workers,
            pin_memory=True
        )

    return {
        # è¿™é‡Œè°ƒç”¨æ—¶ç”¨äº† sampler=samplerï¼Œæ‰€ä»¥ä¸Šé¢çš„å®šä¹‰å¿…é¡»æ˜¯ sampler=None
        'train': make_loader(data_dict['train'], sampler=sampler, shuffle=False),
        'val': make_loader(data_dict['val']),
        'test': make_loader(data_dict['test'])
    }


# ==========================================
# 4. æ”¹è¿›ç‰ˆè¯„ä¼°å‡½æ•° (åˆ†æ®µç»Ÿè®¡)
# ==========================================
def evaluate_model_segmented(model, loader, source_scaler):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()

    src_true, src_pred = [], []
    dist_true, dist_pred = [], []

    print("ğŸ§ª æ­£åœ¨è¿›è¡Œåˆ†æ®µè¯„ä¼°...")
    with torch.no_grad():
        for batch in loader:
            x, eng, y_src, y_dist, _ = [t.to(device) for t in batch]
            s_out, d_out, _ = model(x, eng)

            src_pred.extend(s_out.cpu().numpy().flatten())
            src_true.extend(y_src.cpu().numpy().flatten())
            dist_pred.extend(d_out.cpu().numpy().flatten())
            dist_true.extend(y_dist.cpu().numpy().flatten())

    # åå½’ä¸€åŒ–
    src_pred_real = np.expm1(source_scaler.inverse_transform(np.array(src_pred).reshape(-1, 1)).flatten())
    src_true_real = np.expm1(source_scaler.inverse_transform(np.array(src_true).reshape(-1, 1)).flatten())
    dist_pred = np.array(dist_pred)
    dist_true = np.array(dist_true)

    # é˜²æ­¢è´Ÿæ•°
    src_pred_real = np.maximum(src_pred_real, 0)
    dist_pred = np.maximum(dist_pred, 0)

    # --- å…¨å±€æŒ‡æ ‡ ---
    from sklearn.metrics import mean_absolute_error, r2_score
    print("\n====== ğŸŒ å…¨å±€æµ‹è¯•æŠ¥å‘Š ======")
    print(
        f"æºå¼º MAE: {mean_absolute_error(src_true_real, src_pred_real):.2f} mg/L (R2: {r2_score(src_true_real, src_pred_real):.3f})")
    print(f"è·ç¦» MAE: {mean_absolute_error(dist_true, dist_pred):.2f} km   (R2: {r2_score(dist_true, dist_pred):.3f})")

    # --- åˆ†æ®µæŒ‡æ ‡ ---
    print("\n====== ğŸ“ åˆ†è·ç¦»æ®µè¯„ä¼° ======")
    bins = [0, 2, 10, 30, 100]
    labels = ["è¿‘åœº (0-2km)", "ä¸­åœº (2-10km)", "è¿œåœº (10-30km)", "è¶…è¿œ (>30km)"]

    indices = np.digitize(dist_true, bins)
    for i in range(1, len(bins)):
        mask = (indices == i)
        if np.sum(mask) > 0:
            d_mae = mean_absolute_error(dist_true[mask], dist_pred[mask])
            d_r2 = r2_score(dist_true[mask], dist_pred[mask])
            print(f"[{labels[i - 1]:<13}] æ ·æœ¬: {np.sum(mask):<5} | è·ç¦»MAE: {d_mae:.2f} km | R2: {d_r2:.2f}")
        else:
            print(f"[{labels[i - 1]:<13}] æ— æ ·æœ¬")


# ==========================================
# 5. ä¸»ç¨‹åº
# ==========================================
def main():
    # 1. å‡†å¤‡æ•°æ®
    try:
        data = prepare_data()
    except Exception as e:
        print(e);
        return

    loaders = create_loaders(data)

    # 2. åˆ›å»ºæ¨¡å‹
    # n_features=4 (COD, pH, DO, Vel)
    model = create_spill_adapted_model(n_features=4, engineered_dim=44)

    # âš ï¸ã€å…³é”®ã€‘è¿™é‡Œä¸è°ƒç”¨ load_pretrained_weightsï¼Œç›´æ¥ä»å¤´è®­ç»ƒ
    print("â„¹ï¸ æç¤º: æœ¬æ¬¡è®­ç»ƒä»é›¶å¼€å§‹ (From Scratch)ï¼Œä¸åŠ è½½æ—§çš„é¢„è®­ç»ƒæƒé‡ã€‚")

    # 3. è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ (æ— äººèˆ¹ç©ºé—´å·¡æµ‹ç‰ˆ)...")
    # å…ˆè·‘ 30 ä¸ª Epoch çœ‹çœ‹æ•ˆæœ
    trained_model = train_spill_model(model, loaders['train'], loaders['val'], epochs=30, lr=1e-3)

    # 4. è¯¦ç»†è¯„ä¼°
    evaluate_model_segmented(trained_model, loaders['test'], data['source_scaler'])


if __name__ == "__main__":
    import multiprocessing

    multiprocessing.freeze_support()
    main()