import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import seaborn as sns
from tqdm import tqdm
import os
import scipy.stats
from sklearn.metrics import r2_score
from copy import deepcopy  #ç”¨äºä¿å­˜bestæ¨¡å‹

#å¯¼å…¥ Mamba
try:
    from mamba_ssm import Mamba
    HAS_MAMBA = True
    print("ğŸ æˆåŠŸåŠ è½½ Mamba æ¨¡å—")
except ImportError:
    HAS_MAMBA = False
    print("âš ï¸ æœªæ£€æµ‹åˆ° mamba_ssmï¼Œå°†ä½¿ç”¨ LSTM æ›¿è¡¥")

#å¯¼å…¥ Efficient-KAN
try:
    from efficient_kan import KAN
    HAS_KAN = True
    print("ğŸ•¸ï¸ æˆåŠŸåŠ è½½ Efficient-KAN æ¨¡å—")
except ImportError:
    HAS_KAN = False
    print("âŒ æœªæ£€æµ‹åˆ° efficient_kanï¼Œè¯·ç¡®ä¿å·²å®‰è£… (pip install -e .)")
    # Mock ç±»é˜²æ­¢å´©æºƒ
    class KAN(nn.Module):
        def __init__(self, layers):
            super().__init__()
            self.layers = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])
        def forward(self, x):
            for layer in self.layers: x = torch.relu(layer(x))
            return x

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 3. æ•°æ®é›† (9ç»´ç‰©ç†ç‰¹å¾)
class PhysicsInformedDataset(Dataset):
    def __init__(self, npz_path):
        data = np.load(npz_path)
        self.raw_X = data['X']
        self.y_dist = torch.FloatTensor(data['y_dist'])
        self.y_mass = torch.log1p(torch.FloatTensor(data.get('y_mass', np.zeros(len(self.y_dist)))))
        
        # ç‰©ç†å‚æ•°
        self.u = torch.FloatTensor(data.get('y_u', np.zeros(len(self.y_dist))))
        self.v_boat = torch.FloatTensor(data.get('y_vboat', np.zeros(len(self.y_dist))))
        self.width = torch.FloatTensor(data.get('y_width', np.full(len(self.y_dist), 15.0)))
        self.depth = torch.FloatTensor(data.get('y_depth', np.full(len(self.y_dist), 1.2)))
        
        # ç»Ÿè®¡ç‰¹å¾
        cod_seqs = self.raw_X[:, 0, :]
        self.kurt = torch.tanh(torch.FloatTensor(scipy.stats.kurtosis(cod_seqs, axis=1)) / 10.0)
        self.skew = torch.tanh(torch.FloatTensor(scipy.stats.skew(cod_seqs, axis=1)) / 5.0)
        self.log_max_cod = torch.FloatTensor(np.log1p(np.max(cod_seqs, axis=1))) / 12.0
        self.log_std_cod = torch.FloatTensor(np.log1p(np.std(cod_seqs, axis=1))) / 8.0

    def __len__(self):
        return len(self.raw_X)

    def __getitem__(self, idx):
        sample = self.raw_X[idx]
        x_img = torch.FloatTensor(np.vstack([
            np.log1p(np.maximum(sample[0, :], 0)) / 12.0,
            (sample[1, :] - 7.0) / 2.0,
            (sample[2, :] - 8.0) / 4.0
        ])).float()

        stats = torch.stack([
            self.u[idx], self.v_boat[idx], self.u[idx]-self.v_boat[idx],
            self.kurt[idx], self.skew[idx], self.log_max_cod[idx], self.log_std_cod[idx],
            self.width[idx]/20.0, self.depth[idx]/2.0
        ]).float()

        return x_img, stats, torch.log10(self.y_dist[idx]), self.y_mass[idx]

# æ¨¡å‹ç»„ä»¶: SE-Block & Mamba
class SEBlock(nn.Module):
    def __init__(self, channel, reduction=4):
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )
    def forward(self, x):
        b, l, c = x.size()
        y = x.permute(0, 2, 1)
        y = self.avg_pool(y).view(b, c)
        return x * self.fc(y).view(b, 1, c)

class SEMambaBlock(nn.Module):
    def __init__(self, d_model, d_state=16, expand=2):
        super().__init__()
        if HAS_MAMBA:
            self.mamba = Mamba(d_model=d_model, d_state=d_state, expand=expand)
        else:
            self.lstm = nn.LSTM(d_model, d_model//2, batch_first=True, bidirectional=True)
        self.norm = nn.LayerNorm(d_model)
        self.se = SEBlock(d_model)

    def forward(self, x):
        res = x
        x = self.norm(x)
        if HAS_MAMBA:
            x = self.mamba(x)
        else:
            x, _ = self.lstm(x)
        x = self.se(x)
        return x + res

# æ ¸å¿ƒæ¨¡å‹: PI-KAN-Mamba
class PI_KAN_Mamba(nn.Module):
    def __init__(self, d_model=64):
        super().__init__()
        
        # Branch 1: Sequence Encoder (Mamba)
        self.input_proj = nn.Sequential(nn.Linear(3, d_model), nn.GELU())
        self.mamba_layers = nn.Sequential(
            SEMambaBlock(d_model),
            SEMambaBlock(d_model),
            SEMambaBlock(d_model)
        )
        
        # Branch 2: Physics Encoder (KAN)
        # è¾“å…¥9ç»´ -> éšè—32 -> è¾“å‡º32
        self.phys_encoder = KAN([9, 32, 32])
        
        # Fusion Head
        self.head = nn.Sequential(
            nn.Linear(d_model+32, 64), nn.GELU(), nn.Dropout(0.1),
            nn.Linear(64, 32), nn.GELU(),
            nn.Linear(32, 3) # [Log_Dist, Log_Sigma, Log_Mass]
        )

    def forward(self, x, stats):
        x_emb = self.input_proj(x.permute(0, 2, 1))
        x_out = self.mamba_layers(x_emb)
        seq_feat = torch.mean(x_out, dim=1)
        
        phys_feat = self.phys_encoder(stats) 
        
        combined = torch.cat([seq_feat, phys_feat], dim=1)
        return self.head(combined)

# é«˜çº§å¯è§†åŒ–å·¥å…· (å« KAN å¯è§£é‡Šæ€§)
class Visualizer:
    def plot_performance(self, trues, preds, r2):
        """ç”»æœ€ç»ˆçš„é¢„æµ‹æ€§èƒ½å›¾"""
        mae = np.mean(np.abs(preds - trues))
        
        fig = plt.figure(figsize=(14, 6))
        gs = GridSpec(1, 2, width_ratios=[1, 1])
        
        # æ•£ç‚¹å›¾
        ax1 = fig.add_subplot(gs[0])
        sns.regplot(x=trues, y=preds, ax=ax1, scatter_kws={'alpha':0.5, 's':10, 'color':'teal'}, line_kws={'color':'red'})
        ax1.plot([0, 12], [0, 12], 'k--', lw=2)
        ax1.set_xlabel('True Distance (km)', fontweight='bold')
        ax1.set_ylabel('Predicted Distance (km)', fontweight='bold')
        ax1.set_title(f'Prediction Scatter (MAE={mae:.3f}km, RÂ²={r2:.3f})', fontweight='bold')
        ax1.grid(True, alpha=0.3)
        
        # è¯¯å·®åˆ†å¸ƒå›¾
        ax2 = fig.add_subplot(gs[1])
        errors = preds - trues
        sns.histplot(errors, bins=50, kde=True, ax=ax2, color='crimson')
        ax2.set_xlabel('Prediction Error (km)', fontweight='bold')
        ax2.set_title('Error Distribution', fontweight='bold')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('performance_metrics.pdf', dpi=300)
        print("ğŸ“Š æ€§èƒ½å›¾å·²ä¿å­˜: performance_metrics.pdf")
        plt.show()

    def plot_kan_internals(self, model):
        """å¯è§†åŒ– KAN å†…éƒ¨å­¦åˆ°çš„å‡½æ•°å½¢çŠ¶"""
        model.eval()
        # è·å–ç¬¬ä¸€å±‚ KAN
        try:
            kan_layer = model.phys_encoder.layers[0]
        except:
            print("âš ï¸ æ— æ³•è§£æ KAN ç»“æ„ï¼Œè·³è¿‡å¯è§†åŒ–")
            return

        param_names = ['U', 'v_boat', 'v_rel', 'Kurt', 'Skew', 'LogMax', 'LogStd', 'Width', 'Depth']
        x_range = torch.linspace(-1, 1, 100).to(device)
        
        fig, axes = plt.subplots(3, 3, figsize=(12, 10))
        axes = axes.flatten()
        
        print("\nğŸ” è§£æ KAN å†…éƒ¨å‡½æ•°å½¢çŠ¶...")
        for i in range(9):
            ax = axes[i]
            # æ„é€ è¾“å…¥ï¼šåªæœ‰ç¬¬ i ç»´å˜åŒ–ï¼Œå…¶ä»–ä¸º 0
            input_tensor = torch.zeros(100, 9).to(device)
            input_tensor[:, i] = x_range
            
            with torch.no_grad():
                # è·å–è¾“å‡ºçš„å‰å‡ ä¸ªç»´åº¦
                output = model.phys_encoder(input_tensor)
                for j in range(min(3, output.shape[1])):
                    ax.plot(x_range.cpu(), output[:, j].cpu(), alpha=0.7, label=f'Out_{j}')
            
            ax.set_title(f'Learned f(x) for {param_names[i]}', fontsize=10)
            ax.set_xlabel(param_names[i])
            ax.grid(True, alpha=0.3)
        
        plt.suptitle('KAN Interpretability: Learned Physical Activation Functions', fontweight='bold')
        plt.tight_layout()
        plt.savefig('kan_interpretability.pdf', dpi=300)
        print("ğŸ“Š KANå¯è§£é‡Šæ€§å›¾å·²ä¿å­˜: kan_interpretability.pdf")
        plt.show()

    def plot_kan_comparison(self, kan_full, kan_no_phys):
        mlp_full = 1.40; mlp_no_phys = 5.88
        fig, ax = plt.subplots(figsize=(8, 5))
        
        labels = ['Full Model', 'Ablation (No Phys)']
        x = np.arange(len(labels)); width = 0.35
        
        ax.bar(x - width/2, [mlp_full, mlp_no_phys], width, label='MLP Baseline', color='#A23B72', alpha=0.8, edgecolor='k')
        ax.bar(x + width/2, [kan_full, kan_no_phys], width, label='KAN (Ours)', color='#2E86AB', alpha=0.9, edgecolor='k')
        
        ax.set_ylabel('MAE (km)'); ax.set_title('KAN vs. MLP Impact')
        ax.set_xticks(x); ax.set_xticklabels(labels)
        ax.legend()
        plt.tight_layout()
        plt.savefig('kan_vs_mlp.pdf', dpi=300)
        plt.show()
    
    def plot_training_history(self, history):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(1, 2, figsize=(12, 4))
        
        # MAEæ›²çº¿
        ax1 = axes[0]
        ax1.plot(history['val_mae'], 'b-', linewidth=2, label='Val MAE')
        if history['best_epoch'] is not None:
            ax1.axvline(x=history['best_epoch'], color='r', linestyle='--', label=f'Best (Ep {history["best_epoch"]+1})')
            ax1.scatter([history['best_epoch']], [history['best_mae']], color='r', s=100, zorder=5)
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('MAE (km)')
        ax1.set_title('Validation MAE over Training')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Lossæ›²çº¿
        ax2 = axes[1]
        ax2.plot(history['train_loss'], 'g-', linewidth=2)
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Train Loss')
        ax2.set_title('Training Loss')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('training_history.pdf', dpi=300)
        print("ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜: training_history.pdf")
        plt.show()

# è¯„ä¼°å·¥å…·
def calculate_metrics(trues, preds):
    mae = np.mean(np.abs(preds - trues))
    r2 = r2_score(trues, preds)
    
    errors = np.abs(preds - trues)
    near_mask = trues < 3.0
    mid_mask = (trues >= 3.0) & (trues < 8.0)
    far_mask = trues >= 8.0
    
    mae_near = np.mean(errors[near_mask]) if np.any(near_mask) else 0
    mae_mid = np.mean(errors[mid_mask]) if np.any(mid_mask) else 0
    mae_far = np.mean(errors[far_mask]) if np.any(far_mask) else 0
    
    return mae, r2, mae_near, mae_mid, mae_far

# ä¸»è®­ç»ƒç¨‹åºï¼ˆå¸¦æ—©åœå’ŒBestæ¨¡å‹ä¿å­˜ï¼‰
def train_kan_mamba():
    if not os.path.exists('ultimate_dataset_v3.npz'):
        print("âŒ æ‰¾ä¸åˆ°æ•°æ®é›†"); return
    
    ds = PhysicsInformedDataset('ultimate_dataset_v3.npz')
    train_len = int(0.8 * len(ds))
    train_ds, val_ds = torch.utils.data.random_split(ds, [train_len, len(ds) - train_len])
    
    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=64)
    
    model = PI_KAN_Mamba().to(device)
    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
    criterion_nll = nn.GaussianNLLLoss(reduction='none')
    criterion_mass = nn.SmoothL1Loss()
    visualizer = Visualizer()
    
    #æ—©åœé…ç½®
    MAX_EPOCHS = 100          # æœ€å¤§è®­ç»ƒè½®æ•°
    PATIENCE = 15             # æ—©åœè€å¿ƒå€¼ï¼šè¿ç»­15è½®ä¸æ”¹å–„å°±åœæ­¢
    best_mae = float('inf')   # è®°å½•æœ€ä½³MAE
    best_model_state = None   # ä¿å­˜æœ€ä½³æ¨¡å‹æƒé‡
    patience_counter = 0      # è€å¿ƒè®¡æ•°å™¨
    
    # è®­ç»ƒå†å²è®°å½•
    history = {
        'train_loss': [],
        'val_mae': [],
        'val_r2': [],
        'best_epoch': None,
        'best_mae': None
    }
    
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ PI-KAN-Mamba")
    print(f"   æœ€å¤§è½®æ•°: {MAX_EPOCHS} | æ—©åœè€å¿ƒ: {PATIENCE}")
    print("="*60)
    
    for epoch in range(MAX_EPOCHS):
        # ========== è®­ç»ƒé˜¶æ®µ ==========
        model.train()
        train_loss = 0
        loop = tqdm(train_loader, desc=f"Ep {epoch+1:3d}", leave=False)
        for x, stats, y_d_log, y_m in loop:
            x, stats, y_d_log, y_m = x.to(device), stats.to(device), y_d_log.to(device), y_m.to(device)
            optimizer.zero_grad()
            
            pred = model(x, stats)
            dist_mu, dist_log_var, mass_mu = pred[:, 0], pred[:, 1], pred[:, 2]
            
            weights = 1.0 + 2.0 * torch.exp(-0.5 * torch.pow(10, y_d_log))
            loss = torch.mean(criterion_nll(dist_mu, y_d_log, torch.exp(dist_log_var)) * weights) * 10.0 + \
                   criterion_mass(mass_mu, y_m) * 0.5
            
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        avg_train_loss = train_loss / len(train_loader)
        
        # ========== éªŒè¯é˜¶æ®µ ==========
        model.eval()
        all_preds, all_trues = [], []
        with torch.no_grad():
            for x, stats, y_d_log, _ in val_loader:
                x, stats = x.to(device), stats.to(device)
                p = torch.pow(10, model(x, stats)[:, 0]).cpu().numpy()
                t = torch.pow(10, y_d_log).cpu().numpy()
                all_preds.extend(p); all_trues.extend(t)
        
        all_preds = np.array(all_preds); all_trues = np.array(all_trues)
        mae, r2, near, mid, far = calculate_metrics(all_trues, all_preds)
        
        # è®°å½•å†å²
        history['train_loss'].append(avg_train_loss)
        history['val_mae'].append(mae)
        history['val_r2'].append(r2)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯Best
        if mae < best_mae:
            best_mae = mae
            best_model_state = deepcopy(model.state_dict())
            patience_counter = 0
            history['best_epoch'] = epoch
            history['best_mae'] = mae
            marker = " âœ“ BEST"
            # ç«‹å³ä¿å­˜bestæ¨¡å‹
            torch.save(best_model_state, 'best_pi_kan_mamba.pth')
        else:
            patience_counter += 1
            marker = ""
        
        # æ‰“å°ä¿¡æ¯
        print(f"[Ep {epoch+1:3d}] Loss: {avg_train_loss:.4f} | MAE: {mae:.3f} km | RÂ²: {r2:.3f} | "
              f"Near: {near:.2f} | Mid: {mid:.2f} | Far: {far:.2f}{marker}")
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(mae)
        
        # æ—©åœæ£€æŸ¥
        if patience_counter >= PATIENCE:
            print(f"\nâ¹ï¸ æ—©åœè§¦å‘ï¼è¿ç»­ {PATIENCE} è½®éªŒè¯MAEæœªæ”¹å–„")
            print(f"   æœ€ä½³æ¨¡å‹åœ¨ Epoch {history['best_epoch']+1}ï¼ŒMAE = {best_mae:.3f} km")
            break
    
    # æ¢å¤Bestæ¨¡å‹
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
        print(f"\nâœ… å·²æ¢å¤æœ€ä½³æ¨¡å‹ (Epoch {history['best_epoch']+1}, MAE={best_mae:.3f} km)")
    
    # æœ€ç»ˆè¯„ä¼°ï¼ˆç”¨bestæ¨¡å‹ï¼‰
    model.eval()
    all_preds, all_trues = [], []
    with torch.no_grad():
        for x, stats, y_d_log, _ in val_loader:
            x, stats = x.to(device), stats.to(device)
            p = torch.pow(10, model(x, stats)[:, 0]).cpu().numpy()
            t = torch.pow(10, y_d_log).cpu().numpy()
            all_preds.extend(p); all_trues.extend(t)
    
    all_preds = np.array(all_preds); all_trues = np.array(all_trues)
    mae, r2, near, mid, far = calculate_metrics(all_trues, all_preds)
    
    print("\n" + "="*60)
    print("ğŸ“Š æœ€ç»ˆç»“æœ (Best Model)")
    print(f"   MAE: {mae:.3f} km | RÂ²: {r2:.3f}")
    print(f"   Near (<3km): {near:.3f} | Mid (3-8km): {mid:.3f} | Far (>8km): {far:.3f}")
    print("="*60)

    #å¯è§†åŒ–
    print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    visualizer.plot_training_history(history)
    visualizer.plot_performance(all_trues, all_preds, r2)
    visualizer.plot_kan_internals(model)
    
    #æ¶ˆèå®éªŒ
    print("\nğŸ”¬ è¿è¡Œæ¶ˆèå®éªŒ...")
    model.eval()
    errs_full, errs_no = [], []
    with torch.no_grad():
        for x, stats, y_d_log, _ in val_loader:
            x, stats, y_d_log = x.to(device), stats.to(device), y_d_log.to(device)
            t = torch.pow(10, y_d_log).cpu().numpy()
            p1 = torch.pow(10, model(x, stats)[:, 0]).cpu().numpy()
            p2 = torch.pow(10, model(x, torch.zeros_like(stats))[:, 0]).cpu().numpy()
            errs_full.extend(np.abs(p1 - t))
            errs_no.extend(np.abs(p2 - t))
    
    visualizer.plot_kan_comparison(np.mean(errs_full), np.mean(errs_no))
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹ 
    torch.save(model.state_dict(), 'agent_model_kan_mamba.pth')
    print("\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜:")
    print("   - best_pi_kan_mamba.pth (è®­ç»ƒè¿‡ç¨‹ä¸­çš„æœ€ä½³)")
    print("   - agent_model_kan_mamba.pth (æœ€ç»ˆæ¨¡å‹ï¼Œä¸bestç›¸åŒ)")


if __name__ == "__main__":
    train_kan_mamba()