"""
è®ºæ–‡çº§å¯¹æ¯”å›¾ï¼šOurs vs LSTM / Transformer
ç”Ÿæˆä¸¤å¼ ç²¾ç¾çš„å¯¹æ¯”å›¾
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from matplotlib.patches import FancyBboxPatch
import seaborn as sns
from scipy import stats
import os

# è®¾ç½®ç»˜å›¾é£æ ¼
plt.rcParams.update({
    'font.size': 11,
    'font.family': 'serif',
    'axes.labelsize': 12,
    'axes.titlesize': 13,
    'xtick.labelsize': 10,
    'ytick.labelsize': 10,
    'legend.fontsize': 10,
    'figure.dpi': 150,
    'savefig.dpi': 300,
    'axes.linewidth': 1.2,
})


def load_results():
    """
    åŠ è½½å®éªŒç»“æœ
    """
    # å°è¯•åŠ è½½ä¹‹å‰å®éªŒä¿å­˜çš„ç»“æœ
    if os.path.exists('experiment_results.npz'):
        data = np.load('experiment_results.npz', allow_pickle=True)
        return data['results'].item()
    
    # å¦‚æœæ²¡æœ‰ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç»“æ„ï¼ˆè¿è¡Œablation_experiment.pyåä¼šæœ‰çœŸå®æ•°æ®ï¼‰
    print("âš ï¸ æœªæ‰¾åˆ°å®éªŒç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ ablation_experiment.py")
    print("   æˆ–è€…å°†æ­¤ä»£ç æ”¾åœ¨å®éªŒä»£ç åé¢ä¸€èµ·è¿è¡Œ")
    return None


def create_ours_vs_lstm_figure(ours_preds, ours_trues, lstm_preds, lstm_trues, save_path='ours_vs_lstm.pdf'):
    """
    åˆ›å»º Ours vs LSTM çš„å¯¹æ¯”å›¾
    """
    fig = plt.figure(figsize=(14, 10))
    gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1], 
                           hspace=0.28, wspace=0.25)
    
    # è®¡ç®—æŒ‡æ ‡
    ours_mae = np.mean(np.abs(ours_preds - ours_trues))
    lstm_mae = np.mean(np.abs(lstm_preds - lstm_trues))
    ours_errors = np.abs(ours_preds - ours_trues)
    lstm_errors = np.abs(lstm_preds - lstm_trues)
    
    # é¢œè‰²æ–¹æ¡ˆ
    ours_color = '#E74C3C'  # çº¢è‰²
    lstm_color = '#7F8C8D'  # ç°è‰²
    
    # ========== (a) æ•£ç‚¹å›¾å¯¹æ¯” ==========
    ax1 = fig.add_subplot(gs[0, 0])
    
    # é‡‡æ ·ç‚¹
    n_samples = min(600, len(ours_trues))
    idx = np.random.choice(len(ours_trues), n_samples, replace=False)
    
    # å…ˆç”»LSTMï¼ˆç°è‰²èƒŒæ™¯ï¼‰
    ax1.scatter(lstm_trues[idx], lstm_preds[idx], c=lstm_color, alpha=0.35, 
               s=30, label=f'LSTM (MAE={lstm_mae:.2f}km)', edgecolors='none')
    # å†ç”»Oursï¼ˆçº¢è‰²å‰æ™¯ï¼‰
    ax1.scatter(ours_trues[idx], ours_preds[idx], c=ours_color, alpha=0.7, 
               s=35, label=f'Ours (MAE={ours_mae:.2f}km)', edgecolors='white', linewidth=0.3)
    
    # ç†æƒ³çº¿
    ax1.plot([0, 12], [0, 12], 'k--', lw=2, alpha=0.6, label='Ideal')
    
    ax1.set_xlabel('Ground Truth Distance (km)', fontweight='bold')
    ax1.set_ylabel('Predicted Distance (km)', fontweight='bold')
    ax1.set_title('(a) Prediction Scatter Comparison', fontweight='bold', pad=10)
    ax1.legend(loc='upper left', frameon=True, fancybox=True, shadow=True)
    ax1.set_xlim(0, 12)
    ax1.set_ylim(0, 12)
    ax1.set_aspect('equal')
    ax1.grid(True, alpha=0.3, linestyle='--')
    
    # æ·»åŠ LSTMæ ‡æ³¨
    lstm_mean = np.mean(lstm_preds)
    ax1.axhline(y=lstm_mean, color=lstm_color, linestyle=':', alpha=0.8, lw=1.5)
    ax1.annotate('Higher prediction variance', xy=(9, lstm_mean), 
                xytext=(9.5, lstm_mean + 2),
                fontsize=9, color='#555', fontweight='bold',
                arrowprops=dict(arrowstyle='->', color='#555', lw=1.5))
    
    # ========== (b) è¯¯å·®åˆ†å¸ƒæ›²çº¿ ==========
    ax2 = fig.add_subplot(gs[0, 1])
    
    # æ’åºè¯¯å·®
    ours_sorted = np.sort(ours_errors)
    lstm_sorted = np.sort(lstm_errors)
    percentiles = np.linspace(0, 100, len(ours_sorted))
    
    ax2.fill_between(percentiles, lstm_sorted, ours_sorted, 
                     where=(lstm_sorted > ours_sorted),
                     color=ours_color, alpha=0.15, label='Our Advantage')
    ax2.plot(percentiles, lstm_sorted, color=lstm_color, linewidth=2.5, 
            linestyle='--', label='LSTM', alpha=0.8)
    ax2.plot(percentiles, ours_sorted, color=ours_color, linewidth=3, 
            label='Ours (PI-KAN-Mamba)')
    
    ax2.set_xlabel('Sample Percentile (%)', fontweight='bold')
    ax2.set_ylabel('Absolute Error (km)', fontweight='bold')
    ax2.set_title('(b) Sorted Error Distribution', fontweight='bold', pad=10)
    ax2.legend(loc='upper left', frameon=True, fancybox=True, shadow=True)
    ax2.set_xlim(0, 100)
    ax2.set_ylim(0, 6)
    ax2.grid(True, alpha=0.3, linestyle='--')
    
    # æ ‡æ³¨å…³é”®ç‚¹
    p90_ours = ours_sorted[int(len(ours_sorted)*0.9)]
    p90_lstm = lstm_sorted[int(len(lstm_sorted)*0.9)]
    ax2.annotate(f'90th: {p90_ours:.1f}km', xy=(90, p90_ours), 
                xytext=(75, p90_ours-0.8), fontsize=9, color=ours_color, fontweight='bold',
                arrowprops=dict(arrowstyle='->', color=ours_color, lw=1.2))
    ax2.annotate(f'90th: {p90_lstm:.1f}km', xy=(90, p90_lstm), 
                xytext=(75, p90_lstm+0.5), fontsize=9, color=lstm_color, fontweight='bold',
                arrowprops=dict(arrowstyle='->', color=lstm_color, lw=1.2))
    
    # ========== (c) åˆ†è·ç¦»æ®µMAEæŸ±çŠ¶å›¾ ==========
    ax3 = fig.add_subplot(gs[1, 0])
    
    bins = [(0, 2), (2, 4), (4, 6), (6, 8), (8, 10), (10, 12)]
    bin_labels = ['0-2', '2-4', '4-6', '6-8', '8-10', '10-12']
    
    ours_bin_mae = []
    lstm_bin_mae = []
    for lo, hi in bins:
        mask = (ours_trues >= lo) & (ours_trues < hi)
        ours_bin_mae.append(np.mean(ours_errors[mask]) if mask.sum() > 0 else 0)
        lstm_bin_mae.append(np.mean(lstm_errors[mask]) if mask.sum() > 0 else 0)
    
    x = np.arange(len(bins))
    width = 0.35
    
    bars1 = ax3.bar(x - width/2, lstm_bin_mae, width, label='LSTM', 
                    color=lstm_color, alpha=0.7, edgecolor='black', linewidth=1)
    bars2 = ax3.bar(x + width/2, ours_bin_mae, width, label='Ours', 
                    color=ours_color, alpha=0.85, edgecolor='black', linewidth=1)
    
    # åœ¨æŸ±å­ä¸Šæ ‡æ³¨æ”¹è¿›ç™¾åˆ†æ¯”
    for i, (l, o) in enumerate(zip(lstm_bin_mae, ours_bin_mae)):
        if l > 0:
            improve = (l - o) / l * 100
            ax3.annotate(f'-{improve:.0f}%', xy=(x[i] + width/2, o), 
                        xytext=(x[i] + width/2, o + 0.15),
                        ha='center', fontsize=8, color='#27ae60', fontweight='bold')
    
    ax3.set_xlabel('Distance Range (km)', fontweight='bold')
    ax3.set_ylabel('MAE (km)', fontweight='bold')
    ax3.set_title('(c) MAE by Distance Range', fontweight='bold', pad=10)
    ax3.set_xticks(x)
    ax3.set_xticklabels(bin_labels)
    ax3.legend(loc='upper left', frameon=True, fancybox=True, shadow=True)
    ax3.grid(True, alpha=0.3, axis='y', linestyle='--')
    
    # ========== (d) è¯¯å·®ç®±çº¿å›¾ ==========
    ax4 = fig.add_subplot(gs[1, 1])
    
    # æŒ‰è·ç¦»åˆ†ç»„çš„è¯¯å·®
    ours_grouped = [ours_errors[(ours_trues >= lo) & (ours_trues < hi)] 
                   for lo, hi in bins]
    lstm_grouped = [lstm_errors[(lstm_trues >= lo) & (lstm_trues < hi)] 
                   for lo, hi in bins]
    
    positions_lstm = np.arange(len(bins)) * 2
    positions_ours = positions_lstm + 0.7
    
    bp1 = ax4.boxplot(lstm_grouped, positions=positions_lstm, widths=0.5,
                      patch_artist=True, showfliers=False)
    bp2 = ax4.boxplot(ours_grouped, positions=positions_ours, widths=0.5,
                      patch_artist=True, showfliers=False)
    
    # è®¾ç½®é¢œè‰²
    for patch in bp1['boxes']:
        patch.set_facecolor(lstm_color)
        patch.set_alpha(0.6)
    for patch in bp2['boxes']:
        patch.set_facecolor(ours_color)
        patch.set_alpha(0.8)
    
    ax4.set_xlabel('Distance Range (km)', fontweight='bold')
    ax4.set_ylabel('Absolute Error (km)', fontweight='bold')
    ax4.set_title('(d) Error Distribution by Range', fontweight='bold', pad=10)
    ax4.set_xticks(positions_lstm + 0.35)
    ax4.set_xticklabels(bin_labels)
    ax4.legend([bp1['boxes'][0], bp2['boxes'][0]], ['LSTM', 'Ours'], 
              loc='upper left', frameon=True, fancybox=True, shadow=True)
    ax4.grid(True, alpha=0.3, axis='y', linestyle='--')
    ax4.set_ylim(0, 8)
    
    # æ·»åŠ æ€»æ ‡é¢˜
    fig.suptitle('PI-KAN-Mamba vs. LSTM Baseline', fontsize=16, fontweight='bold', y=0.98)
    
    plt.savefig(save_path, bbox_inches='tight', dpi=300)
    plt.savefig(save_path.replace('.pdf', '.png'), bbox_inches='tight', dpi=150)
    print(f"ğŸ“Š å·²ä¿å­˜: {save_path}")
    plt.show()


def create_ours_vs_transformer_figure(ours_preds, ours_trues, tf_preds, tf_trues, 
                                       save_path='ours_vs_transformer.pdf'):
    """
    åˆ›å»º Ours vs Transformer çš„å¯¹æ¯”å›¾
    """
    fig = plt.figure(figsize=(14, 10))
    gs = gridspec.GridSpec(2, 2, height_ratios=[1, 1], width_ratios=[1, 1], 
                           hspace=0.28, wspace=0.25)
    
    # è®¡ç®—æŒ‡æ ‡
    ours_mae = np.mean(np.abs(ours_preds - ours_trues))
    tf_mae = np.mean(np.abs(tf_preds - tf_trues))
    ours_errors = np.abs(ours_preds - ours_trues)
    tf_errors = np.abs(tf_preds - tf_trues)
    
    # é¢œè‰²æ–¹æ¡ˆ
    ours_color = '#E74C3C'  # çº¢è‰²
    tf_color = '#9B59B6'    # ç´«è‰²
    
    # ========== (a) æ•£ç‚¹å›¾å¯¹æ¯” ==========
    ax1 = fig.add_subplot(gs[0, 0])
    
    n_samples = min(600, len(ours_trues))
    idx = np.random.choice(len(ours_trues), n_samples, replace=False)
    
    # å…ˆç”»Transformerï¼ˆç´«è‰²èƒŒæ™¯ï¼‰
    ax1.scatter(tf_trues[idx], tf_preds[idx], c=tf_color, alpha=0.35, 
               s=30, label=f'Transformer (MAE={tf_mae:.2f}km)', edgecolors='none')
    # å†ç”»Oursï¼ˆçº¢è‰²å‰æ™¯ï¼‰
    ax1.scatter(ours_trues[idx], ours_preds[idx], c=ours_color, alpha=0.7, 
               s=35, label=f'Ours (MAE={ours_mae:.2f}km)', edgecolors='white', linewidth=0.3)
    
    ax1.plot([0, 12], [0, 12], 'k--', lw=2, alpha=0.6, label='Ideal')
    
    ax1.set_xlabel('Ground Truth Distance (km)', fontweight='bold')
    ax1.set_ylabel('Predicted Distance (km)', fontweight='bold')
    ax1.set_title('(a) Prediction Scatter Comparison', fontweight='bold', pad=10)
    ax1.legend(loc='upper left', frameon=True, fancybox=True, shadow=True)
    ax1.set_xlim(0, 12)
    ax1.set_ylim(0, 12)
    ax1.set_aspect('equal')
    ax1.grid(True, alpha=0.3, linestyle='--')
    
    # æ ‡æ³¨Transformerçš„å‘æ•£ç‰¹æ€§
    ax1.annotate('Transformer:\nHigher variance\nat all ranges', 
                xy=(3, 7), xytext=(1, 9),
                fontsize=9, color='#8e44ad', fontweight='bold',
                arrowprops=dict(arrowstyle='->', color='#8e44ad', lw=1.5),
                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))
    
    # ========== (b) è¯¯å·®åˆ†å¸ƒæ›²çº¿ ==========
    ax2 = fig.add_subplot(gs[0, 1])
    
    ours_sorted = np.sort(ours_errors)
    tf_sorted = np.sort(tf_errors)
    percentiles = np.linspace(0, 100, len(ours_sorted))
    
    ax2.fill_between(percentiles, tf_sorted, ours_sorted, 
                     where=(tf_sorted > ours_sorted),
                     color=ours_color, alpha=0.15, label='Our Advantage')
    ax2.plot(percentiles, tf_sorted, color=tf_color, linewidth=2.5, 
            linestyle='--', label='Transformer', alpha=0.8)
    ax2.plot(percentiles, ours_sorted, color=ours_color, linewidth=3, 
            label='Ours (PI-KAN-Mamba)')
    
    ax2.set_xlabel('Sample Percentile (%)', fontweight='bold')
    ax2.set_ylabel('Absolute Error (km)', fontweight='bold')
    ax2.set_title('(b) Sorted Error Distribution', fontweight='bold', pad=10)
    ax2.legend(loc='upper left', frameon=True, fancybox=True, shadow=True)
    ax2.set_xlim(0, 100)
    ax2.set_ylim(0, 8)
    ax2.grid(True, alpha=0.3, linestyle='--')
    
    # æ ‡æ³¨å·®è·
    mid_idx = len(percentiles) // 2
    gap = tf_sorted[mid_idx] - ours_sorted[mid_idx]
    ax2.annotate(f'Median gap:\n{gap:.1f}km', xy=(50, (tf_sorted[mid_idx] + ours_sorted[mid_idx])/2), 
                xytext=(60, tf_sorted[mid_idx] + 1), fontsize=9, fontweight='bold',
                arrowprops=dict(arrowstyle='->', color='black', lw=1.2))
    
    # ========== (c) åˆ†è·ç¦»æ®µMAEæŸ±çŠ¶å›¾ ==========
    ax3 = fig.add_subplot(gs[1, 0])
    
    bins = [(0, 2), (2, 4), (4, 6), (6, 8), (8, 10), (10, 12)]
    bin_labels = ['0-2', '2-4', '4-6', '6-8', '8-10', '10-12']
    
    ours_bin_mae = []
    tf_bin_mae = []
    for lo, hi in bins:
        mask = (ours_trues >= lo) & (ours_trues < hi)
        ours_bin_mae.append(np.mean(ours_errors[mask]) if mask.sum() > 0 else 0)
        tf_bin_mae.append(np.mean(tf_errors[mask]) if mask.sum() > 0 else 0)
    
    x = np.arange(len(bins))
    width = 0.35
    
    bars1 = ax3.bar(x - width/2, tf_bin_mae, width, label='Transformer', 
                    color=tf_color, alpha=0.7, edgecolor='black', linewidth=1)
    bars2 = ax3.bar(x + width/2, ours_bin_mae, width, label='Ours', 
                    color=ours_color, alpha=0.85, edgecolor='black', linewidth=1)
    
    for i, (t, o) in enumerate(zip(tf_bin_mae, ours_bin_mae)):
        if t > 0:
            improve = (t - o) / t * 100
            ax3.annotate(f'-{improve:.0f}%', xy=(x[i] + width/2, o), 
                        xytext=(x[i] + width/2, o + 0.2),
                        ha='center', fontsize=8, color='#27ae60', fontweight='bold')
    
    ax3.set_xlabel('Distance Range (km)', fontweight='bold')
    ax3.set_ylabel('MAE (km)', fontweight='bold')
    ax3.set_title('(c) MAE by Distance Range', fontweight='bold', pad=10)
    ax3.set_xticks(x)
    ax3.set_xticklabels(bin_labels)
    ax3.legend(loc='upper left', frameon=True, fancybox=True, shadow=True)
    ax3.grid(True, alpha=0.3, axis='y', linestyle='--')
    
    # ========== (d) æ€§èƒ½é›·è¾¾å›¾ / ç»¼åˆå¯¹æ¯” ==========
    ax4 = fig.add_subplot(gs[1, 1])
    
    # è®¡ç®—å¤šä¸ªæŒ‡æ ‡
    from sklearn.metrics import r2_score
    
    metrics = ['MAE\n(km)', 'RÂ²', 'Near\n(<3km)', 'Mid\n(3-8km)', 'Far\n(>8km)']
    
    # OursæŒ‡æ ‡
    ours_r2 = r2_score(ours_trues, ours_preds)
    near_mask = ours_trues < 3
    mid_mask = (ours_trues >= 3) & (ours_trues < 8)
    far_mask = ours_trues >= 8
    ours_near = np.mean(ours_errors[near_mask])
    ours_mid = np.mean(ours_errors[mid_mask])
    ours_far = np.mean(ours_errors[far_mask])
    
    # TransformeræŒ‡æ ‡
    tf_r2 = r2_score(tf_trues, tf_preds)
    tf_near = np.mean(tf_errors[near_mask])
    tf_mid = np.mean(tf_errors[mid_mask])
    tf_far = np.mean(tf_errors[far_mask])
    
    # æŸ±çŠ¶å›¾å¯¹æ¯”
    x = np.arange(5)
    width = 0.35
    
    # æ³¨æ„ï¼šRÂ²è¶Šé«˜è¶Šå¥½ï¼Œå…¶ä»–è¶Šä½è¶Šå¥½ï¼Œéœ€è¦ç»Ÿä¸€æ–¹å‘
    # è¿™é‡Œç›´æ¥å±•ç¤ºåŸå§‹å€¼
    ours_vals = [ours_mae, ours_r2, ours_near, ours_mid, ours_far]
    tf_vals = [tf_mae, tf_r2, tf_near, tf_mid, tf_far]
    
    ax4.barh(x - width/2, tf_vals, width, label='Transformer', 
            color=tf_color, alpha=0.7, edgecolor='black')
    ax4.barh(x + width/2, ours_vals, width, label='Ours', 
            color=ours_color, alpha=0.85, edgecolor='black')
    
    # æ ‡æ³¨æ•°å€¼
    for i, (t, o) in enumerate(zip(tf_vals, ours_vals)):
        ax4.text(t + 0.1, x[i] - width/2, f'{t:.2f}', va='center', fontsize=9, color=tf_color)
        ax4.text(o + 0.1, x[i] + width/2, f'{o:.2f}', va='center', fontsize=9, color=ours_color, fontweight='bold')
    
    ax4.set_yticks(x)
    ax4.set_yticklabels(metrics)
    ax4.set_xlabel('Value', fontweight='bold')
    ax4.set_title('(d) Comprehensive Metrics Comparison', fontweight='bold', pad=10)
    ax4.legend(loc='upper right', frameon=True, fancybox=True, shadow=True)
    ax4.grid(True, alpha=0.3, axis='x', linestyle='--')
    ax4.invert_yaxis()
    
    # æ·»åŠ æ€»æ ‡é¢˜
    fig.suptitle('PI-KAN-Mamba vs. Transformer Baseline', fontsize=16, fontweight='bold', y=0.98)
    
    plt.savefig(save_path, bbox_inches='tight', dpi=300)
    plt.savefig(save_path.replace('.pdf', '.png'), bbox_inches='tight', dpi=150)
    print(f"ğŸ“Š å·²ä¿å­˜: {save_path}")
    plt.show()


def main():
    """
    ä¸»å‡½æ•°ï¼šç”Ÿæˆå¯¹æ¯”å›¾
    éœ€è¦åœ¨è¿è¡Œå®Œ ablation_experiment.py åè¿è¡Œï¼Œ
    æˆ–è€…å°†ç»“æœæ•°æ®ä¼ å…¥
    """
    
    print("="*60)
    print("ğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾")
    print("="*60)
    
    # ========== æ–¹æ³•1: ä»å…¨å±€å˜é‡è·å–ï¼ˆå¦‚æœå’Œablationä¸€èµ·è¿è¡Œï¼‰==========
    # å¦‚æœä½ æŠŠè¿™æ®µä»£ç åŠ åˆ° ablation_experiment.py çš„ main() æœ«å°¾ï¼Œ
    # å¯ä»¥ç›´æ¥ä½¿ç”¨ results å˜é‡
    
    # ========== æ–¹æ³•2: æ‰‹åŠ¨è¾“å…¥ä½ çš„å®éªŒç»“æœ ==========
    # æ ¹æ®ä½ çš„å®éªŒç»“æœå¡«å…¥ï¼ˆä»ablationå®éªŒçš„è¾“å‡ºå¤åˆ¶ï¼‰
    
    print("\nè¯·ç¡®ä¿å·²è¿è¡Œ ablation_experiment.py å¹¶è®°å½•äº†ç»“æœ")
    print("ç°åœ¨ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆç¤ºä¾‹å›¾...")
    
    # æ¨¡æ‹Ÿæ•°æ®ï¼ˆæ›¿æ¢æˆä½ çš„çœŸå®æ•°æ®ï¼‰
    np.random.seed(42)
    n_samples = 2000
    
    # çœŸå®è·ç¦»ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰
    trues = np.random.uniform(0.3, 12, n_samples)
    
    # Oursé¢„æµ‹ï¼ˆæœ€å¥½ï¼‰
    ours_preds = trues + np.random.normal(0, 0.8, n_samples) * (1 + trues/15)
    ours_preds = np.clip(ours_preds, 0.1, 15)
    
    # LSTMé¢„æµ‹ï¼ˆæœ‰mode collapseå€¾å‘ï¼‰
    lstm_preds = trues * 0.6 + np.mean(trues) * 0.4 + np.random.normal(0, 1.5, n_samples)
    lstm_preds = np.clip(lstm_preds, 0.1, 15)
    
    # Transformeré¢„æµ‹ï¼ˆæ–¹å·®å¤§ï¼‰
    tf_preds = trues + np.random.normal(0, 1.8, n_samples) * (1 + trues/10)
    tf_preds = np.clip(tf_preds, 0.1, 15)
    
    # è°ƒæ•´ä»¥åŒ¹é…ä½ çš„å®éªŒMAE
    # Ours: ~1.27, LSTM: ~2.12, Transformer: ~2.65
    
    print(f"\næ¨¡æ‹Ÿæ•°æ® MAE:")
    print(f"  Ours: {np.mean(np.abs(ours_preds - trues)):.2f} km")
    print(f"  LSTM: {np.mean(np.abs(lstm_preds - trues)):.2f} km")
    print(f"  Transformer: {np.mean(np.abs(tf_preds - trues)):.2f} km")
    
    # ç”Ÿæˆå›¾è¡¨
    print("\n" + "="*60)
    create_ours_vs_lstm_figure(ours_preds, trues, lstm_preds, trues)
    
    print("\n" + "="*60)
    create_ours_vs_transformer_figure(ours_preds, trues, tf_preds, trues)
    
    print("\nâœ… å®Œæˆï¼ç”Ÿæˆäº†ä¸¤å¼ å¯¹æ¯”å›¾ï¼š")
    print("   - ours_vs_lstm.pdf/png")
    print("   - ours_vs_transformer.pdf/png")


def generate_from_results(results):
    """
    ä»ablationå®éªŒçš„resultså­—å…¸ç”Ÿæˆå›¾è¡¨
    åœ¨ablation_experiment.pyçš„main()æœ«å°¾è°ƒç”¨ï¼š
    
    from comparison_figures import generate_from_results
    generate_from_results(results)
    """
    ours = results.get('Ours (Mamba+KAN)', {})
    lstm = results.get('Pure LSTM', {})
    tf = results.get('Pure Transformer', {})
    
    if ours and lstm:
        create_ours_vs_lstm_figure(
            ours['preds'], ours['trues'],
            lstm['preds'], lstm['trues']
        )
    
    if ours and tf:
        create_ours_vs_transformer_figure(
            ours['preds'], ours['trues'],
            tf['preds'], tf['trues']
        )


if __name__ == "__main__":
    main()