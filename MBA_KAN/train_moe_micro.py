import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import os
import scipy.stats

# è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸš€ Training Device: {device}")


# ==========================================
# 1. æ•°æ®é›†: ç‰©ç†å‚æ•°åˆ†ç¦»ç‰ˆ
# ==========================================
class PhysicsInformedDataset(Dataset):
    def __init__(self, npz_path):
        try:
            data = np.load(npz_path)
            print(f"ğŸ“‚ æˆåŠŸåŠ è½½æ•°æ®é›†: {npz_path}")
        except FileNotFoundError:
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ•°æ®é›†: {npz_path}")

        self.raw_X = data['X']
        self.y_dist = torch.FloatTensor(data['y_dist'])

        # å¤„ç† Mass
        if 'y_mass' in data:
            self.y_mass = torch.log1p(torch.FloatTensor(data['y_mass']))
        else:
            self.y_mass = torch.zeros_like(self.y_dist)

        # åŠ è½½ç‰©ç†å‚æ•°
        if 'y_u' in data and 'y_vboat' in data:
            self.u = torch.FloatTensor(data['y_u'])
            self.v_boat = torch.FloatTensor(data['y_vboat'])
            self.width = torch.FloatTensor(data['y_width'])
            self.depth = torch.FloatTensor(data['y_depth'])
            print("âœ… ç‰©ç†å‚æ•° (U, Boat, Width, Depth) å·²å…¨éƒ¨åŠ è½½")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°ç‰©ç†å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            self.u = torch.zeros_like(self.y_dist)
            self.v_boat = torch.zeros_like(self.y_dist)
            self.width = torch.full_like(self.y_dist, 10.0)  # é»˜è®¤å€¼
            self.depth = torch.full_like(self.y_dist, 1.0)

        # é¢„è®¡ç®—ç»Ÿè®¡ç‰¹å¾
        print("âš¡ é¢„è®¡ç®—æ³¢å½¢ç»Ÿè®¡ç‰¹å¾...")
        cod_seqs = self.raw_X[:, 0, :]
        self.kurt = torch.FloatTensor(scipy.stats.kurtosis(cod_seqs, axis=1))
        self.skew = torch.FloatTensor(scipy.stats.skew(cod_seqs, axis=1))
        # Tanh å‹ç¼©é˜²æ­¢æ•°å€¼çˆ†ç‚¸
        self.kurt = torch.tanh(self.kurt / 10.0)
        self.skew = torch.tanh(self.skew / 5.0)

        max_vals = np.max(cod_seqs, axis=1)
        std_vals = np.std(cod_seqs, axis=1)
        # Log å˜æ¢
        self.log_max_cod = torch.FloatTensor(np.log1p(max_vals)) / 12.0  # é€‚é… v4 æ•°æ®
        self.log_std_cod = torch.FloatTensor(np.log1p(std_vals)) / 8.0

    def __len__(self):
        return len(self.raw_X)

    def __getitem__(self, idx):
        sample = self.raw_X[idx].copy()

        # --- 1. ç‰©ç†å‚æ•° ---
        u_val = self.u[idx]
        v_boat_val = self.v_boat[idx]
        v_rel = u_val - v_boat_val  # ç›¸å¯¹é€Ÿåº¦

        # --- 2. å›¾åƒé€šé“ (ä»…æ°´è´¨, å»æ‰é€Ÿåº¦é€šé“) ---
        cod_raw = sample[0, :]
        # Global Log Normalization
        cod_norm = np.log1p(np.maximum(cod_raw, 0)) / 12.0

        ph_norm = (sample[1, :] - 7.0) / 2.0
        do_norm = (sample[2, :] - 8.0) / 4.0

        # åªå †å  3 ä¸ªé€šé“ [COD, pH, DO]
        # é€Ÿåº¦ä¿¡æ¯é€šè¿‡ stats ä¼ å…¥ï¼Œä¸å†å¹²æ‰° CNN è§†çº¿
        x_img = torch.FloatTensor(np.vstack([cod_norm, ph_norm, do_norm])).float()

        # --- 3. ç‰©ç†ä¸Šä¸‹æ–‡ (Physics Context) ---
        stats = torch.stack([
            u_val,
            v_boat_val,
            torch.tensor(v_rel, dtype=torch.float),  # å…³é”®: æ˜¾å¼ä¼ å…¥ç›¸å¯¹é€Ÿåº¦
            self.kurt[idx],
            self.skew[idx],
            self.log_max_cod[idx],
            self.log_std_cod[idx],
            self.width[idx] / 20.0,  # å½’ä¸€åŒ–å¤„ç†
            self.depth[idx] / 2.0
        ]).float()

        # --- 4. æ ‡ç­¾ (Log Space) ---
        target_dist_log = torch.log10(self.y_dist[idx])

        return x_img, stats, target_dist_log, self.y_mass[idx]


# ==========================================
# 2. æ¨¡å‹: PI-Attentive Net (SE-Block + Physics Fusion)
# ==========================================
class SEBlock(nn.Module):
    """ Squeeze-and-Excitation Attention Block """

    def __init__(self, channel, reduction=16):
        super(SEBlock, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1)
        return x * y.expand_as(x)


class PI_Attentive_MoE(nn.Module):
    def __init__(self):
        super().__init__()

        # Branch 1: CNN + Attention (æå–æ³¢å½¢ç‰¹å¾)
        # Input: [Batch, 3, 30]
        self.cnn = nn.Sequential(
            nn.Conv1d(3, 32, kernel_size=5, padding=2),
            nn.BatchNorm1d(32), nn.ReLU(),
            SEBlock(32, reduction=4),  # Attention!
            nn.MaxPool1d(2),

            nn.Conv1d(32, 64, kernel_size=3, padding=1),
            nn.BatchNorm1d(64), nn.ReLU(),
            SEBlock(64, reduction=8),  # Attention!
            nn.MaxPool1d(2),

            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm1d(128), nn.ReLU(),
            nn.AdaptiveAvgPool1d(1)
        )  # Output: [Batch, 128, 1]

        # Branch 2: Physics Encoder (å¤„ç†æµé€Ÿç­‰å‚æ•°)
        # Input: 7 dims
        self.phys_encoder = nn.Sequential(
            nn.Linear(9, 32),
            nn.BatchNorm1d(32), nn.ReLU(),
            nn.Linear(32, 32),
            nn.ReLU()
        )

        # Fusion Head
        # 128 (CNN) + 32 (Physics) = 160
        self.head = nn.Sequential(
            nn.Linear(160, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            # [ä¿®æ”¹] è¾“å‡ºæ”¹ä¸º 3 ç»´:
            # 0: Log_Dist_Mu (è·ç¦»é¢„æµ‹)
            # 1: Log_Dist_Sigma (è·ç¦»ä¸ç¡®å®šåº¦)
            # 2: Log_Mass_Mu (æºå¼ºé¢„æµ‹ - è¾…åŠ©ä»»åŠ¡)
            nn.Linear(64, 3)
        )

    def forward(self, x, stats):
        # 1. Image Branch
        cnn_feat = self.cnn(x)
        cnn_feat = cnn_feat.view(cnn_feat.size(0), -1)  # Flatten -> [B, 128]

        # 2. Physics Branch
        phys_feat = self.phys_encoder(stats)  # -> [B, 32]

        # 3. Injection
        combined = torch.cat([cnn_feat, phys_feat], dim=1)
        return self.head(combined)


# ==========================================
# 3. è¯Šæ–­å‡½æ•°: æ•æ„Ÿåº¦æµ‹è¯•
# ==========================================
def run_sensitivity_test(model, val_loader):
    """
    è¯Šæ–­æ¨¡å‹æ˜¯å¦çœŸçš„åœ¨çœ‹æ³¢å³°ï¼š
    éšæœºå–ä¸€ä¸ªè¿‘åœºæ ·æœ¬ï¼ŒæŠŠå®ƒçš„æ³¢å³°å¼ºè¡ŒæŠ¹å¹³ï¼Œçœ‹é¢„æµ‹è·ç¦»æ˜¯å¦å˜å¤§ã€‚
    """
    model.eval()
    print("\nğŸ” [è¯Šæ–­] æ­£åœ¨è¿›è¡Œæ³¢å³°æ•æ„Ÿåº¦æµ‹è¯• (Peak Sensitivity Test)...")

    # æ‰¾ä¸€ä¸ªè¿‘åœºæ ·æœ¬ (<1.0km)
    target_sample = None
    target_stats = None
    target_dist = None

    for x, stats, y_d_log, _ in val_loader:
        real_dist = torch.pow(10, y_d_log)
        mask = real_dist < 1.0
        if mask.any():
            idx = torch.where(mask)[0][0]
            target_sample = x[idx:idx + 1].clone().to(device)
            target_stats = stats[idx:idx + 1].clone().to(device)
            target_dist = real_dist[idx].item()
            break

    if target_sample is None:
        print("âš ï¸ éªŒè¯é›†ä¸­æ²¡æ‰¾åˆ°è¿‘åœºæ ·æœ¬ï¼Œè·³è¿‡æµ‹è¯•ã€‚")
        return

    # 1. åŸå§‹é¢„æµ‹
    with torch.no_grad():
        pred_orig = model(target_sample, target_stats)
        dist_orig = torch.pow(10, pred_orig[0, 0]).item()

    # 2. æŠ¹å¹³æ³¢å³° (æŠŠ COD é€šé“ç½®ä¸º 0)
    modified_sample = target_sample.clone()
    modified_sample[:, 0, :] = 0.0  # Kill the peak!

    # ä¹Ÿè¦æŠŠ stats é‡Œçš„ max_cod æŠ¹æ‰ï¼Œä¸ç„¶æ¨¡å‹ä¼šä» stats é‡Œå·çœ‹
    modified_stats = target_stats.clone()
    modified_stats[:, 5] = 0.0  # log_max_cod = 0
    modified_stats[:, 6] = 0.0  # log_std_cod = 0

    with torch.no_grad():
        pred_mod = model(modified_sample, modified_stats)
        dist_mod = torch.pow(10, pred_mod[0, 0]).item()

    print(f"   æ ·æœ¬çœŸå®è·ç¦»: {target_dist:.2f} km")
    print(f"   [1] åŸå§‹é¢„æµ‹: {dist_orig:.2f} km")
    print(f"   [2] æŠ¹å¹³æ³¢å³°å: {dist_mod:.2f} km")

    change = dist_mod - dist_orig
    if change > 2.0:
        print("âœ… è¯Šæ–­é€šè¿‡ï¼šæ¨¡å‹é€šè¿‡æ³¢å³°åˆ¤æ–­è·ç¦» (æŠ¹å¹³æ³¢å³°å¯¼è‡´é¢„æµ‹è·ç¦»å‰§å¢)")
    else:
        print("âŒ è¯Šæ–­è­¦å‘Šï¼šæ¨¡å‹å¯¹æ³¢å³°ä¸æ•æ„Ÿï¼å¯èƒ½ä¾ç„¶åœ¨çŒœæ¦‚ç‡ã€‚")
    print("-" * 50)


# ==========================================
# 4. ä¸»è®­ç»ƒå¾ªç¯
# ==========================================
def train_pi_attentive():
    if os.path.exists('ultimate_dataset_v3.npz'):
        data_path = 'ultimate_dataset_v3.npz'
        print(f"ğŸ“‚ ä½¿ç”¨æ•°æ®é›†: {data_path}")
    else:
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®é›†ï¼Œè¯·å…ˆè¿è¡Œ generate_micro_data.py")
        return

    BATCH_SIZE = 64
    EPOCHS = 15

    ds = PhysicsInformedDataset(data_path)
    train_len = int(0.8 * len(ds))
    train_ds, val_ds = torch.utils.data.random_split(ds, [train_len, len(ds) - train_len])

    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, num_workers=0)

    model = PI_Attentive_MoE().to(device)

    # âš ï¸ å…³é”®: reduction='none' å…è®¸æˆ‘ä»¬æ‰‹åŠ¨åŠ æƒ
    criterion_nll = nn.GaussianNLLLoss(reduction='none')
    criterion_mass = nn.SmoothL1Loss(reduction='mean')   # Mass ç”¨å›å½’

    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)

    print("ğŸš€ å¼€å§‹è®­ç»ƒ PI-Attentive Net (å¸¦åŠ æƒLoss)...")

    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0

        loop = tqdm(train_loader, desc=f"Ep {epoch + 1}/{EPOCHS}", leave=False)

        for x, stats, y_d_log, y_m in loop:
            x, stats = x.to(device), stats.to(device)
            y_d_log, y_m = y_d_log.to(device), y_m.to(device)

            optimizer.zero_grad()
            pred = model(x, stats)
            dist_mu = pred[:, 0]  # è·ç¦»å‡å€¼
            dist_log_var = pred[:, 1]  # è·ç¦»æ–¹å·®çš„log
            mass_mu = pred[:, 2]  # æºå¼ºé¢„æµ‹

            # 1. è·ç¦» Loss (NLL + åŠ æƒ)
            dist_var = torch.exp(dist_log_var)
            with torch.no_grad():
                true_dist_km = torch.pow(10, y_d_log)
                weights = 1.0 + 2.0 * torch.exp(-0.5 * true_dist_km)  # è¿‘åœºåŠ æƒ

            raw_loss_d = criterion_nll(dist_mu, y_d_log, dist_var)
            loss_d = torch.mean(raw_loss_d * weights)

            # 2. æºå¼º Loss (è¾…åŠ©ä»»åŠ¡)
            # è¿™ä¼šå¼ºè¿«æ¨¡å‹å»ç†è§£"ç°åœ¨çš„æµ“åº¦é«˜æ˜¯å› ä¸ºè·ç¦»è¿‘ï¼Œè¿˜æ˜¯å› ä¸ºæºå¼º å¤§"
            loss_m = criterion_mass(mass_mu, y_m)

            # 3. æ€» Loss
            # ç»™ Mass ä»»åŠ¡ 0.5 çš„æƒé‡ï¼Œè®©å®ƒè¾…åŠ©ä¸»ä»»åŠ¡
            loss = loss_d * 10.0 + loss_m * 0.5

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            train_loss += loss.item()
            loop.set_postfix(loss=loss.item())

        # === éªŒè¯ & è¯Šæ–­ ===
        model.eval()
        val_loss = 0
        all_preds, all_trues = [], []
        errors = {'near': [], 'mid': [], 'far': []}

        with torch.no_grad():
            for x, stats, y_d_log, y_m in val_loader:
                x, stats = x.to(device), stats.to(device)
                y_d_log = y_d_log.to(device)

                pred = model(x, stats)
                mu = pred[:, 0]

                # è®¡ç®—éªŒè¯ Loss (ä¸åŠ æƒï¼Œçœ‹åŸå§‹è¡¨ç°)
                val_loss += nn.SmoothL1Loss()(pred[:, 0], y_d_log).item()

                pred_km = torch.pow(10, mu).cpu().numpy()
                true_km = torch.pow(10, y_d_log).cpu().numpy()

                all_preds.extend(pred_km)
                all_trues.extend(true_km)

                abs_err = np.abs(pred_km - true_km)
                for i, d in enumerate(true_km):
                    if d < 3.0:
                        errors['near'].append(abs_err[i])
                    elif d < 8.0:
                        errors['mid'].append(abs_err[i])
                    else:
                        errors['far'].append(abs_err[i])

        # æ‰“å°æŒ‡æ ‡
        mae_near = np.mean(errors['near']) if errors['near'] else 0
        mae_mid = np.mean(errors['mid']) if errors['mid'] else 0
        mae_far = np.mean(errors['far']) if errors['far'] else 0
        total_mae = np.mean(np.abs(np.array(all_preds) - np.array(all_trues)))

        print(f"Ep {epoch + 1}: Val Loss={val_loss / len(val_loader):.4f} | Avg MAE={total_mae:.2f}km")
        print(f"      ğŸ“ Near: {mae_near:.2f} | Mid: {mae_mid:.2f} | Far: {mae_far:.2f}")

        # è¿è¡Œæ•æ„Ÿåº¦è¯Šæ–­
        if (epoch + 1) % 5 == 0:
            run_sensitivity_test(model, val_loader)

        scheduler.step(val_loss)

    # === æœ€ç»ˆç”»å›¾ ===
    print("\nğŸ“Š ç”Ÿæˆæœ€ç»ˆç»“æœå›¾...")
    all_trues = np.array(all_trues)
    all_preds = np.array(all_preds)

    plt.figure(figsize=(14, 6))

    # æ•£ç‚¹å›¾
    plt.subplot(1, 2, 1)
    plt.scatter(all_trues, all_preds, alpha=0.5, s=10, c='teal', label='Predictions')
    plt.plot([0, 12], [0, 12], 'k--', lw=2)
    plt.title(f"Probabilistic Regression (MAE={total_mae:.2f}km)")
    plt.xlabel("True Distance (km)")
    plt.ylabel("Predicted Distance (km)")
    plt.grid(True, alpha=0.3)

    # è¯Šæ–­å›¾: è·ç¦» vs è¯¯å·®
    # æˆ‘ä»¬å¸Œæœ›çœ‹åˆ°è¿‘åœº(å·¦è¾¹)çš„è¯¯å·®å¾ˆä½ï¼Œè€Œä¸æ˜¯å¾ˆé«˜
    plt.subplot(1, 2, 2)
    abs_errors = np.abs(all_preds - all_trues)
    plt.scatter(all_trues, abs_errors, alpha=0.5, s=10, c='crimson')
    plt.hlines(0.5, 0, 12, colors='k', linestyles='dashed', label='0.5km Error')
    plt.title("Diagnosis: Distance vs. Absolute Error")
    plt.xlabel("True Distance (km)")
    plt.ylabel("Absolute Error (km)")
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()
    return model


if __name__ == "__main__":
    model = train_pi_attentive()
    torch.save(model.state_dict(), 'agent_model_final.pth')
    print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: agent_model_final.pth")